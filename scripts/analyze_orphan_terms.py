#!/usr/bin/env python3
"""
ê³ ì•„ ìš©ì–´ ë¶„ì„ ìŠ¤í¬ë¦½íŠ¸
- ê´€ê³„ê°€ ì „í˜€ ì—†ëŠ” ìš©ì–´ë“¤ì„ ì°¾ì•„ì„œ ë¶„ì„
- ë”ë¯¸ ë°ì´í„° vs ì‹¤ì œ ì¤‘ìš” ìš©ì–´ êµ¬ë¶„
"""

from src.infrastructure.supabase_client import SupabaseClient
from collections import defaultdict
import json

def analyze_orphan_terms():
    """ê³ ì•„ ìš©ì–´ ë¶„ì„"""
    supabase = SupabaseClient()

    print("=" * 70)
    print("ê³ ì•„ ìš©ì–´(Orphan Terms) ë¶„ì„")
    print("=" * 70)
    print()

    # 1. ì „ì²´ ìš©ì–´ ë¡œë“œ (í˜ì´ì§€ë„¤ì´ì…˜)
    print("ğŸ“Š ìš©ì–´ ë°ì´í„° ë¡œë”© ì¤‘...")
    all_terms = []
    page = 0
    page_size = 1000

    while True:
        response = supabase.client.table('playbook_semantic_terms').select(
            'id,doc_id,term,category,definition,frequency,confidence,raw_relations'
        ).range(page * page_size, (page + 1) * page_size - 1).execute()

        if not response.data:
            break

        all_terms.extend(response.data)
        page += 1

        if len(response.data) < page_size:
            break

    print(f"âœ“ ë¡œë“œ ì™„ë£Œ: {len(all_terms)}ê°œ ìš©ì–´\n")

    # 2. ê´€ê³„ ë°ì´í„° ë¡œë“œ
    print("ğŸ”— ê´€ê³„ ë°ì´í„° ë¡œë”© ì¤‘...")
    relations = supabase.client.table('playbook_graph_relations').select(
        'source_term_id,target_term_id'
    ).execute()

    print(f"âœ“ ë¡œë“œ ì™„ë£Œ: {len(relations.data)}ê°œ ê´€ê³„\n")

    # 3. ê´€ê³„ê°€ ìˆëŠ” ìš©ì–´ ID ì§‘í•©
    connected_term_ids = set()
    for rel in relations.data:
        connected_term_ids.add(rel['source_term_id'])
        connected_term_ids.add(rel['target_term_id'])

    # 4. ê³ ì•„ ìš©ì–´ í•„í„°ë§
    orphan_terms = []
    terms_with_raw_relations = []

    for term in all_terms:
        if term['id'] not in connected_term_ids:
            orphan_terms.append(term)

            # raw_relationsê°€ ìˆëŠ”ì§€ í™•ì¸
            if term.get('raw_relations'):
                terms_with_raw_relations.append(term)

    # 5. í†µê³„ ì¶œë ¥
    print("=" * 70)
    print("ğŸ“ˆ ì „ì²´ í†µê³„")
    print("=" * 70)
    print(f"ì´ ìš©ì–´ ìˆ˜: {len(all_terms):,}ê°œ")
    print(f"ì—°ê²°ëœ ìš©ì–´: {len(connected_term_ids):,}ê°œ ({len(connected_term_ids)/len(all_terms)*100:.1f}%)")
    print(f"ê³ ì•„ ìš©ì–´: {len(orphan_terms):,}ê°œ ({len(orphan_terms)/len(all_terms)*100:.1f}%)")
    print(f"  - raw_relations ìˆìŒ: {len(terms_with_raw_relations):,}ê°œ ({len(terms_with_raw_relations)/len(orphan_terms)*100:.1f}%)")
    print(f"  - raw_relations ì—†ìŒ: {len(orphan_terms) - len(terms_with_raw_relations):,}ê°œ")
    print()

    # 6. ì¹´í…Œê³ ë¦¬ë³„ ê³ ì•„ ìš©ì–´ ë¶„ì„
    orphan_by_category = defaultdict(int)
    orphan_with_raw_by_category = defaultdict(int)

    for term in orphan_terms:
        orphan_by_category[term['category']] += 1
        if term.get('raw_relations'):
            orphan_with_raw_by_category[term['category']] += 1

    print("=" * 70)
    print("ğŸ“Š ì¹´í…Œê³ ë¦¬ë³„ ê³ ì•„ ìš©ì–´ ë¶„í¬")
    print("=" * 70)
    print(f"{'ì¹´í…Œê³ ë¦¬':<15} {'ê³ ì•„ ìˆ˜':>10} {'raw_relations':>15} {'ë³€í™˜ ì‹¤íŒ¨ìœ¨':>15}")
    print("-" * 70)

    for category in sorted(orphan_by_category.keys(), key=lambda x: orphan_by_category[x], reverse=True):
        total = orphan_by_category[category]
        with_raw = orphan_with_raw_by_category[category]
        failure_rate = (with_raw / total * 100) if total > 0 else 0
        print(f"{category:<15} {total:>10} {with_raw:>15} {failure_rate:>14.1f}%")

    print()

    # 7. ê³ ì•„ ìš©ì–´ ìƒ˜í”Œ (raw_relations ìˆëŠ” ê²ƒ ìš°ì„ )
    print("=" * 70)
    print("ğŸ” ê³ ì•„ ìš©ì–´ ìƒ˜í”Œ (raw_relations ìˆìŒ)")
    print("=" * 70)
    print("ì´ ìš©ì–´ë“¤ì€ LLMì´ ê´€ê³„ë¥¼ ì¶”ì¶œí–ˆì§€ë§Œ ì˜¨í†¨ë¡œì§€ ê·œì¹™ì´ ì—†ì–´ì„œ ì—°ê²° ì‹¤íŒ¨")
    print()

    for i, term in enumerate(terms_with_raw_relations[:20], 1):
        print(f"{i}. [{term['category']}] {term['term']}")
        print(f"   ì •ì˜: {term.get('definition', 'N/A')[:80]}...")
        print(f"   ë¹ˆë„: {term.get('frequency', 0)}, ì‹ ë¢°ë„: {term.get('confidence', 0):.2f}")
        print(f"   raw_relations: {len(term.get('raw_relations', []))}ê°œ")

        # raw_relations ìƒ˜í”Œ ì¶œë ¥
        if term.get('raw_relations'):
            for j, raw_rel in enumerate(term['raw_relations'][:3], 1):
                print(f"      {j}) {raw_rel.get('relation_type', 'N/A')} -> {raw_rel.get('target', 'N/A')}")
        print()

    # 8. ì§„ì§œ ê³ ì•„ ìš©ì–´ (raw_relationsë„ ì—†ìŒ)
    real_orphans = [t for t in orphan_terms if not t.get('raw_relations')]

    print("=" * 70)
    print("ğŸš« ì§„ì§œ ê³ ì•„ ìš©ì–´ (raw_relations ì—†ìŒ)")
    print("=" * 70)
    print(f"ì´ {len(real_orphans):,}ê°œ - ì´ë“¤ì€ LLMì´ ê´€ê³„ë¥¼ ì¶”ì¶œí•˜ì§€ ëª»í•œ ìš©ì–´")
    print()

    # ë¹ˆë„ë³„ ì •ë ¬ (ë†’ì€ ë¹ˆë„ = ì¤‘ìš”í•  ê°€ëŠ¥ì„±)
    real_orphans_sorted = sorted(real_orphans, key=lambda x: x.get('frequency', 0), reverse=True)

    print("ë¹ˆë„ê°€ ë†’ì€ ì§„ì§œ ê³ ì•„ ìš©ì–´ TOP 20:")
    print("-" * 70)

    for i, term in enumerate(real_orphans_sorted[:20], 1):
        print(f"{i}. [{term['category']}] {term['term']}")
        print(f"   ë¹ˆë„: {term.get('frequency', 0)}, ì‹ ë¢°ë„: {term.get('confidence', 0):.2f}")
        print(f"   ì •ì˜: {term.get('definition', 'N/A')[:80]}...")
        print()

    # 9. ê¶Œì¥ ì‚¬í•­
    print("=" * 70)
    print("ğŸ’¡ ê¶Œì¥ ì‚¬í•­")
    print("=" * 70)
    print()
    print(f"1. raw_relations ìˆëŠ” ê³ ì•„ ìš©ì–´ ({len(terms_with_raw_relations):,}ê°œ):")
    print("   â†’ ì˜¨í†¨ë¡œì§€ ê·œì¹™ ì¶”ê°€ í•„ìš” (add_missing_ontology_rules.sql ì‹¤í–‰)")
    print()
    print(f"2. ì§„ì§œ ê³ ì•„ ìš©ì–´ ({len(real_orphans):,}ê°œ):")
    print("   a) ë¹ˆë„ ë†’ìŒ (frequency >= 3): Phase 1 í”„ë¡¬í”„íŠ¸ ê°œì„  í•„ìš”")
    print(f"      â†’ {len([t for t in real_orphans if t.get('frequency', 0) >= 3]):,}ê°œ")
    print()
    print("   b) ë¹ˆë„ ë‚®ìŒ (frequency < 3): ë…¸ì´ì¦ˆ ë°ì´í„° ê°€ëŠ¥ì„±")
    print(f"      â†’ {len([t for t in real_orphans if t.get('frequency', 0) < 3]):,}ê°œ")
    print("      â†’ ì‚­ì œ ê³ ë ¤ ë˜ëŠ” ì‹ ë¢°ë„ ì„ê³„ê°’ ìƒí–¥")
    print()

    # 10. í’ˆì§ˆ ë©”íŠ¸ë¦­
    total_raw_relations = sum(len(t.get('raw_relations', [])) for t in all_terms)
    connected_raw = total_raw_relations - sum(len(t.get('raw_relations', [])) for t in terms_with_raw_relations)

    print("=" * 70)
    print("ğŸ“Š í’ˆì§ˆ ë©”íŠ¸ë¦­")
    print("=" * 70)
    print(f"ì „ì²´ raw_relations: {total_raw_relations:,}ê°œ")
    print(f"ì„±ê³µì ìœ¼ë¡œ ë³€í™˜: {len(relations.data):,}ê°œ")
    print(f"ë³€í™˜ ì‹¤íŒ¨: {total_raw_relations - len(relations.data):,}ê°œ")
    print(f"ë³€í™˜ìœ¨: {len(relations.data)/total_raw_relations*100:.1f}%")
    print()
    print(f"ëª©í‘œ ë³€í™˜ìœ¨: 30-50%")
    print(f"í˜„ì¬ ê°­: {30 - len(relations.data)/total_raw_relations*100:.1f}%p")
    print()

if __name__ == '__main__':
    analyze_orphan_terms()
