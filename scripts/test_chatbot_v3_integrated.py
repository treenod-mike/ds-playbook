#!/usr/bin/env python3
"""
v3.0 GraphRAG ì±—ë´‡ (RAG Answer Generator í†µí•©)
- test_chatbot_v2.py + rag_answer_generator.py í†µí•©
- Evidence-based ë‹µë³€ ìƒì„±
- XML êµ¬ì¡°í™”ëœ ì»¨í…ìŠ¤íŠ¸
- 6ë‹¨ê³„ ì¶”ë¡  ê³¼ì • ì‹œê°í™” ìœ ì§€
"""
import sys
import os
from pathlib import Path

project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from src.shared.config import Config
from src.core.generators.rag_answer_generator import (
    RAGContextFormatter,
    RAGAnswerGenerator,
    SearchResult,
    GraphRelation
)
from supabase import create_client
from openai import OpenAI
from collections import defaultdict
import json

# Color codes for terminal output
class Colors:
    HEADER = '\033[95m'
    OKBLUE = '\033[94m'
    OKCYAN = '\033[96m'
    OKGREEN = '\033[92m'
    WARNING = '\033[93m'
    FAIL = '\033[91m'
    ENDC = '\033[0m'
    BOLD = '\033[1m'
    UNDERLINE = '\033[4m'


class GraphRAGChatbotV3:
    """GraphRAG ê¸°ë°˜ ëŒ€í™”í˜• ì±—ë´‡ v3.0 (RAG Generator í†µí•©)"""

    def __init__(self):
        """ì´ˆê¸°í™”"""
        print(f"\n{Colors.HEADER}{'='*70}")
        print("PokoPoko v3.0 GraphRAG ì±—ë´‡ (Evidence-based Answer Generation)")
        print(f"{'='*70}{Colors.ENDC}\n")

        # Supabase ì—°ê²°
        print("ğŸ“¡ Supabase ì—°ê²° ì¤‘...")
        self.supabase = create_client(Config.SUPABASE_URL, Config.SUPABASE_KEY)
        print(f"{Colors.OKGREEN}âœ… Supabase ì—°ê²° ì™„ë£Œ{Colors.ENDC}\n")

        # OpenAI ì—°ê²°
        print("ğŸ¤– OpenAI ì—°ê²° ì¤‘...")
        self.openai_client = OpenAI(
            api_key=os.getenv("OPENAI_API_KEY"),
            base_url=os.getenv("OPENAI_BASE_URL", None)
        )
        print(f"{Colors.OKGREEN}âœ… OpenAI ì—°ê²° ì™„ë£Œ{Colors.ENDC}\n")

        # RAG ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”
        self.formatter = RAGContextFormatter()
        self.generator = RAGAnswerGenerator(self.openai_client)

        # ëŒ€í™” íˆìŠ¤í† ë¦¬
        self.conversation_history = []
        self.context_terms = set()  # ëŒ€í™”ì—ì„œ ì–¸ê¸‰ëœ ìš©ì–´ë“¤

        # ë°ì´í„° ë¡œë“œ
        self._load_ontology_data()

    def _load_ontology_data(self):
        """ì˜¨í†¨ë¡œì§€ ë°ì´í„° ë¡œë“œ"""
        print("ğŸ“š ì˜¨í†¨ë¡œì§€ ë°ì´í„° ë¡œë“œ ì¤‘...")

        # ëª¨ë“  ìš©ì–´ ë¡œë“œ
        terms_result = self.supabase.table('playbook_semantic_terms')\
            .select("id, term, category, definition")\
            .execute()
        self.all_terms = terms_result.data
        self.term_map = {t['term']: t for t in self.all_terms}

        # ì˜¨í†¨ë¡œì§€ ë£° ë¡œë“œ
        rules_result = self.supabase.table('playbook_ontology_rules')\
            .select("subject_type, predicate, object_type, description")\
            .execute()
        self.ontology_rules = rules_result.data

        print(f"{Colors.OKGREEN}âœ… ìš©ì–´ {len(self.all_terms)}ê°œ, ì˜¨í†¨ë¡œì§€ ë£° {len(self.ontology_rules)}ê°œ ë¡œë“œ{Colors.ENDC}\n")

    def find_related_terms(self, user_message):
        """ì‚¬ìš©ì ë©”ì‹œì§€ì—ì„œ ê´€ë ¨ ìš©ì–´ ì¶”ì¶œ"""
        mentioned_terms = []
        seen_terms = set()

        for term_data in self.all_terms:
            if term_data['term'] in user_message:
                term_key = f"{term_data['term']}_{term_data['category']}"
                if term_key not in seen_terms:
                    seen_terms.add(term_key)
                    mentioned_terms.append(term_data)
                    self.context_terms.add(term_data['term'])

        return mentioned_terms

    def get_subgraph(self, center_term, radius=2):
        """ì¤‘ì‹¬ ìš©ì–´ ê¸°ë°˜ ì„œë¸Œê·¸ë˜í”„ ì¶”ì¶œ (hop ê²½ë¡œ ì¶”ì )"""
        # ì¤‘ì‹¬ ìš©ì–´ì˜ ëª¨ë“  ì¸ìŠ¤í„´ìŠ¤ ê°€ì ¸ì˜¤ê¸°
        center_terms = self.supabase.table('playbook_semantic_terms')\
            .select("id, term, category")\
            .eq("term", center_term)\
            .execute()

        if not center_terms.data:
            return {
                "nodes": [],
                "edges": [],
                "reasoning_chain": [],
                "hop_paths": [],
                "traversal_log": [],
                "chunks": []
            }

        center_ids = [t['id'] for t in center_terms.data]

        # BFSë¡œ ê´€ê³„ íƒìƒ‰ (hop ê²½ë¡œ ì¶”ì )
        visited_nodes = {}
        visited_edges = {}
        hop_paths = []
        traversal_log = []

        queue = [(center_id, 0, [center_term]) for center_id in center_ids]
        hop_count = {0: 0, 1: 0, 2: 0}

        while queue:
            current_id, depth, path = queue.pop(0)

            if depth >= radius:
                continue

            # Outgoing edges (source) - evidence ì •ë³´ í¬í•¨
            outgoing = self.supabase.table('playbook_semantic_relations')\
                .select("id, source_term_id, target_term_id, predicate, confidence, evidence, evidence_chunk_id")\
                .eq("source_term_id", current_id)\
                .gte("confidence", 0.5)\
                .execute()

            for edge in outgoing.data:
                edge_key = f"{edge['source_term_id']}_{edge['predicate']}_{edge['target_term_id']}"
                if edge_key not in visited_edges:
                    visited_edges[edge_key] = edge
                    hop_count[depth + 1] += 1

                    # Get target term name
                    target_node = self.supabase.table('playbook_semantic_terms')\
                        .select("term, category")\
                        .eq("id", edge['target_term_id'])\
                        .limit(1)\
                        .execute()

                    if target_node.data:
                        target_term = target_node.data[0]['term']
                        new_path = path + [target_term]

                        hop_paths.append({
                            "hop": depth + 1,
                            "path": " â†’ ".join(new_path),
                            "predicate": edge['predicate'],
                            "confidence": edge['confidence']
                        })

                        queue.append((edge['target_term_id'], depth + 1, new_path))

            # Incoming edges (target) - evidence ì •ë³´ í¬í•¨
            incoming = self.supabase.table('playbook_semantic_relations')\
                .select("id, source_term_id, target_term_id, predicate, confidence, evidence, evidence_chunk_id")\
                .eq("target_term_id", current_id)\
                .gte("confidence", 0.5)\
                .execute()

            for edge in incoming.data:
                edge_key = f"{edge['source_term_id']}_{edge['predicate']}_{edge['target_term_id']}"
                if edge_key not in visited_edges:
                    visited_edges[edge_key] = edge
                    hop_count[depth + 1] += 1

                    # Get source term name
                    source_node = self.supabase.table('playbook_semantic_terms')\
                        .select("term, category")\
                        .eq("id", edge['source_term_id'])\
                        .limit(1)\
                        .execute()

                    if source_node.data:
                        source_term = source_node.data[0]['term']
                        new_path = path + [source_term]

                        hop_paths.append({
                            "hop": depth + 1,
                            "path": " â†’ ".join(new_path),
                            "predicate": edge['predicate'],
                            "confidence": edge['confidence']
                        })

                        queue.append((edge['source_term_id'], depth + 1, new_path))

        # Traversal log
        traversal_log = [
            f"Hop 0 (ì‹œì‘): {center_term}",
            f"Hop 1: {hop_count[1]}ê°œ ê´€ê³„ ë°œê²¬",
            f"Hop 2: {hop_count[2]}ê°œ ê´€ê³„ ë°œê²¬"
        ]

        # ë…¸ë“œ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
        all_node_ids = set()
        for edge in visited_edges.values():
            all_node_ids.add(edge['source_term_id'])
            all_node_ids.add(edge['target_term_id'])

        if not all_node_ids:
            return {
                "nodes": [],
                "edges": [],
                "reasoning_chain": [],
                "hop_paths": [],
                "traversal_log": traversal_log,
                "chunks": []
            }

        nodes_result = self.supabase.table('playbook_semantic_terms')\
            .select("id, term, category, source_chunks")\
            .in_("id", list(all_node_ids))\
            .execute()

        node_map = {n['id']: n for n in nodes_result.data}

        # [ì¶”ê°€] 1. ìˆ˜ì§‘ëœ ì—£ì§€ë“¤ì—ì„œ evidence_chunk_id ì¶”ì¶œ
        chunk_ids_from_evidence = set()
        for edge in visited_edges.values():
            if edge.get('evidence_chunk_id'):
                chunk_ids_from_evidence.add(edge['evidence_chunk_id'])

        # [ì¶”ê°€] 2. ì‹¤ì œ ì²­í¬ í…ìŠ¤íŠ¸ ì¡°íšŒ
        evidence_map = {}
        if chunk_ids_from_evidence:
            try:
                chunks_result = self.supabase.table('playbook_chunks')\
                    .select("id, content, metadata, doc_id")\
                    .in_("id", list(chunk_ids_from_evidence))\
                    .execute()

                for c in chunks_result.data:
                    # ë©”íƒ€ë°ì´í„°ì—ì„œ ì œëª© ì¶”ì¶œ ì‹œë„
                    title = c.get('metadata', {}).get('title', 'Unknown Doc') if isinstance(c.get('metadata'), dict) else 'Unknown Doc'
                    # ë‚´ìš© ë¯¸ë¦¬ë³´ê¸° (100ì)
                    content_preview = c['content'][:100] + "..." if len(c['content']) > 100 else c['content']
                    evidence_map[str(c['id'])] = f"[{title}] {content_preview}"
            except Exception as e:
                # ì²­í¬ ì¡°íšŒ ì‹¤íŒ¨í•´ë„ ê³„ì† ì§„í–‰
                print(f"   âš ï¸ Evidence ì²­í¬ ì¡°íšŒ ì‹¤íŒ¨: {e}")

        # [ìˆ˜ì •] 3. Build reasoning chain with evidence text
        unique_edges = {}
        for edge in visited_edges.values():
            source_term = node_map.get(edge['source_term_id'], {}).get('term', '')
            target_term = node_map.get(edge['target_term_id'], {}).get('term', '')

            if source_term and target_term:
                edge_key = f"{source_term}_{edge['predicate']}_{target_term}"
                if edge_key not in unique_edges or edge['confidence'] > unique_edges[edge_key]['confidence']:
                    # Evidence í…ìŠ¤íŠ¸ ë§¤í•‘
                    evidence_text = ""
                    if edge.get('evidence'):
                        # DBì— evidence í…ìŠ¤íŠ¸ê°€ ìˆìœ¼ë©´ ì‚¬ìš©
                        evidence_text = edge['evidence']
                    elif edge.get('evidence_chunk_id'):
                        # ì²­í¬ì—ì„œ ì¡°íšŒí•œ í…ìŠ¤íŠ¸ ì‚¬ìš©
                        evidence_text = evidence_map.get(str(edge['evidence_chunk_id']), "")

                    unique_edges[edge_key] = {
                        'source': source_term,
                        'predicate': edge['predicate'],
                        'target': target_term,
                        'confidence': edge['confidence'],
                        'evidence_text': evidence_text,  # LLMì—ê²Œ ì „ë‹¬
                        'evidence_chunk_id': edge.get('evidence_chunk_id')
                    }

        reasoning_chain = [
            f"{e['source']} â†’ [{e['predicate']}] â†’ {e['target']} (ì‹ ë¢°ë„: {e['confidence']:.2f})"
            for e in sorted(unique_edges.values(), key=lambda x: x['confidence'], reverse=True)[:10]
        ]

        # ê´€ë ¨ ì²­í¬ ìˆ˜ì§‘ (Vector Search ëŒ€ì‹ )
        chunks = []
        collected_chunk_ids = set()
        for node in nodes_result.data[:5]:  # ìƒìœ„ 5ê°œ ë…¸ë“œ
            if node.get('source_chunks'):
                for chunk_id in node['source_chunks'][:2]:  # ë…¸ë“œë‹¹ 2ê°œ ì²­í¬
                    if chunk_id not in collected_chunk_ids:
                        collected_chunk_ids.add(chunk_id)
                        chunks.append(chunk_id)

        return {
            "nodes": nodes_result.data,
            "edges": list(visited_edges.values()),
            "unique_edges": list(unique_edges.values()),
            "reasoning_chain": reasoning_chain,
            "hop_paths": sorted(hop_paths, key=lambda x: (x['hop'], -x['confidence']))[:15],
            "traversal_log": traversal_log,
            "chunks": chunks
        }

    def _convert_chunks_to_search_results(self, chunk_ids):
        """ì²­í¬ ID ëª©ë¡ì„ SearchResult ê°ì²´ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜"""
        if not chunk_ids:
            return []

        results = []
        for chunk_id in chunk_ids[:5]:  # ìµœëŒ€ 5ê°œ
            chunk_result = self.supabase.table('playbook_chunks')\
                .select("chunk_id, doc_id, content")\
                .eq("chunk_id", chunk_id)\
                .limit(1)\
                .execute()

            if chunk_result.data:
                chunk = chunk_result.data[0]

                # ë¬¸ì„œ ì œëª© ê°€ì ¸ì˜¤ê¸°
                doc_result = self.supabase.table('playbook_documents')\
                    .select("title")\
                    .eq("doc_id", chunk['doc_id'])\
                    .limit(1)\
                    .execute()

                doc_title = doc_result.data[0]['title'] if doc_result.data else "Unknown"

                results.append(SearchResult(
                    chunk_id=chunk['chunk_id'],
                    doc_id=chunk['doc_id'],
                    doc_title=doc_title,
                    content=chunk['content'],
                    similarity=0.85  # ì‹œë®¬ë ˆì´ì…˜ (ì‹¤ì œë¡œëŠ” ì„ë² ë”© ìœ ì‚¬ë„)
                ))

        return results

    def _convert_edges_to_graph_relations(self, subgraph):
        """ì„œë¸Œê·¸ë˜í”„ì˜ edgesë¥¼ GraphRelation ê°ì²´ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜ (evidence í¬í•¨)"""
        relations = []

        # unique_edges ì‚¬ìš© (ì¤‘ë³µ ì œê±°ëœ ê´€ê³„)
        for edge in subgraph.get('unique_edges', [])[:10]:  # ìµœëŒ€ 10ê°œ
            relations.append(GraphRelation(
                source=edge['source'],
                predicate=edge['predicate'],
                target=edge['target'],
                confidence=edge['confidence'],
                evidence=edge.get('evidence_text', '')  # Evidence í…ìŠ¤íŠ¸ í¬í•¨
            ))

        return relations

    def chat(self, user_message):
        """ëŒ€í™” ì²˜ë¦¬ (v3.0: RAG Generator í†µí•©)"""
        print(f"\n{Colors.HEADER}{'='*70}")
        print(f"[ê²€ìƒ‰ ë° ì¶”ë¡  í”„ë¡œì„¸ìŠ¤]")
        print(f"{'='*70}{Colors.ENDC}\n")

        # 1. ê´€ë ¨ ìš©ì–´ ì¶”ì¶œ
        print(f"{Colors.OKCYAN}1ï¸âƒ£ ìš©ì–´ ë§¤ì¹­ ë‹¨ê³„{Colors.ENDC}")
        print(f"   ì‚¬ìš©ì ì§ˆë¬¸: \"{user_message}\"")
        mentioned_terms = self.find_related_terms(user_message)

        if not mentioned_terms:
            print(f"   {Colors.WARNING}âš ï¸ ê´€ë ¨ ìš©ì–´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.{Colors.ENDC}")
            print(f"\n{Colors.FAIL}âŒ ì§ˆë¬¸í•˜ì‹  ìš©ì–´ê°€ DBì— ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.{Colors.ENDC}")
            print("ë‹¤ë¥¸ í‘œí˜„ìœ¼ë¡œ ì‹œë„í•´ë³´ì„¸ìš” (ì˜ˆ: 'ìŠ¤í…Œì´ì§€', 'ë¯¸ì…˜', 'í´ë¡œë²„' ë“±)\n")
            return

        print(f"   {Colors.OKGREEN}âœ… {len(mentioned_terms)}ê°œ ìš©ì–´ ë°œê²¬:{Colors.ENDC}")
        for term in mentioned_terms[:5]:
            print(f"      - {term['term']} ({term['category']})")
        print()

        # 2. ì„œë¸Œê·¸ë˜í”„ ì¶”ì¶œ
        center_term = mentioned_terms[0]['term']
        print(f"{Colors.OKCYAN}2ï¸âƒ£ ê·¸ë˜í”„ íƒìƒ‰ ë‹¨ê³„ (BFS){Colors.ENDC}")
        print(f"   ì¤‘ì‹¬ ìš©ì–´: {center_term}")
        print(f"   íƒìƒ‰ ë°˜ê²½: 2-hop")

        subgraph = self.get_subgraph(center_term, radius=2)

        # Traversal log ì¶œë ¥
        print(f"\n   {Colors.HEADER}[ê·¸ë˜í”„ íƒìƒ‰ ê²½ë¡œ]{Colors.ENDC}")
        for log in subgraph['traversal_log']:
            print(f"   {log}")

        print(f"\n   {Colors.OKGREEN}âœ… ì´ ë…¸ë“œ {len(subgraph['nodes'])}ê°œ, ê´€ê³„ {len(subgraph['edges'])}ê°œ ë°œê²¬{Colors.ENDC}\n")

        if len(subgraph['edges']) == 0:
            print(f"   {Colors.WARNING}âš ï¸ '{center_term}' ìš©ì–´ëŠ” ì¡´ì¬í•˜ì§€ë§Œ ì—°ê²°ëœ ê´€ê³„ê°€ ì—†ìŠµë‹ˆë‹¤.{Colors.ENDC}")
            print("   Phase 2ë¥¼ ì¬ì‹¤í–‰í•˜ê±°ë‚˜ ë‹¤ë¥¸ ìš©ì–´ë¥¼ ì‹œë„í•´ë³´ì„¸ìš”.\n")
            return

        # 3. Hop ê²½ë¡œ ì‹œê°í™”
        print(f"{Colors.OKCYAN}3ï¸âƒ£ Hop ê²½ë¡œ ë¶„ì„{Colors.ENDC}")

        hop1_paths = [p for p in subgraph['hop_paths'] if p['hop'] == 1]
        hop2_paths = [p for p in subgraph['hop_paths'] if p['hop'] == 2]

        if hop1_paths:
            print(f"\n   {Colors.HEADER}[Hop 1 ê²½ë¡œ] (1ë‹¨ê³„ ê´€ê³„, {len(hop1_paths)}ê°œ){Colors.ENDC}")
            for i, path_info in enumerate(hop1_paths[:5], 1):
                print(f"   {i}. {path_info['path']}")
                print(f"      â””â”€ Predicate: {path_info['predicate']} (ì‹ ë¢°ë„: {path_info['confidence']:.2f})")

        if hop2_paths:
            print(f"\n   {Colors.HEADER}[Hop 2 ê²½ë¡œ] (2ë‹¨ê³„ ê´€ê³„, {len(hop2_paths)}ê°œ){Colors.ENDC}")
            for i, path_info in enumerate(hop2_paths[:5], 1):
                print(f"   {i}. {path_info['path']}")
                print(f"      â””â”€ Predicate: {path_info['predicate']} (ì‹ ë¢°ë„: {path_info['confidence']:.2f})")

        # 4. ì¶”ë¡  ì²´ì¸ (ì˜¨í†¨ë¡œì§€ ê¸°ë°˜)
        print(f"\n{Colors.OKCYAN}4ï¸âƒ£ ì˜¨í†¨ë¡œì§€ ê¸°ë°˜ ì¶”ë¡  ì²´ì¸{Colors.ENDC}")
        if subgraph['reasoning_chain']:
            print(f"   {Colors.HEADER}[Top 5 ì¶”ë¡  ê²½ë¡œ]{Colors.ENDC}")
            for i, chain in enumerate(subgraph['reasoning_chain'][:5], 1):
                print(f"   {i}. {chain}")
        print()

        # 5. ì»¨í…ìŠ¤íŠ¸ ìƒì„± (RAG Formatter ì‚¬ìš©)
        print(f"{Colors.OKCYAN}5ï¸âƒ£ RAG ì»¨í…ìŠ¤íŠ¸ ìƒì„± (XML êµ¬ì¡°){Colors.ENDC}")

        # ì²­í¬ ìˆ˜ì§‘
        vector_results = self._convert_chunks_to_search_results(subgraph['chunks'])

        # ê·¸ë˜í”„ ê´€ê³„ ë³€í™˜
        graph_relations = self._convert_edges_to_graph_relations(subgraph)

        context_stats = {
            "ëŒ€í™” ë§¥ë½ ìš©ì–´": len(self.context_terms),
            "ì˜¨í†¨ë¡œì§€ ë£°": len(self.ontology_rules),
            "ì²­í¬ ìˆ˜": len(vector_results),
            "ê·¸ë˜í”„ ê´€ê³„": len(graph_relations)
        }

        print(f"   {Colors.OKGREEN}âœ… ì»¨í…ìŠ¤íŠ¸ ìš”ì†Œ:{Colors.ENDC}")
        for key, value in context_stats.items():
            print(f"      - {key}: {value}ê°œ")
        print()

        # 6. ë‹µë³€ ìƒì„± (RAG Generator ì‚¬ìš©)
        print(f"{Colors.OKCYAN}6ï¸âƒ£ GPT-4 ë‹µë³€ ìƒì„± (Evidence-based){Colors.ENDC}")
        print(f"   ëª¨ë¸: gpt-4o")
        print(f"   ì˜¨í†¨ë¡œì§€ ë£° ê¸°ë°˜ ì¶”ë¡  í™œì„±í™”")
        print(f"   Temperature: 0.3 (ë³´ìˆ˜ì  ìƒì„±)")
        print(f"   ëŒ€í™” íˆìŠ¤í† ë¦¬: {len(self.conversation_history) // 2}í„´ ìœ ì§€\n")

        result = self.generator.generate_answer(
            query=user_message,
            vector_results=vector_results,
            graph_relations=graph_relations,
            ontology_rules=self.ontology_rules,
            center_term=center_term,
            temperature=0.3
        )

        if not result["success"]:
            print(f"{Colors.FAIL}âŒ ë‹µë³€ ìƒì„± ì‹¤íŒ¨: {result['error']}{Colors.ENDC}\n")
            return

        assistant_message = result["answer"]

        # ëŒ€í™” íˆìŠ¤í† ë¦¬ ì—…ë°ì´íŠ¸
        self.conversation_history.append({"role": "user", "content": user_message})
        self.conversation_history.append({"role": "assistant", "content": assistant_message})

        # 7. ìµœì¢… ë‹µë³€ ë° ê·¼ê±°
        print(f"{Colors.HEADER}{'='*70}")
        print(f"[ìµœì¢… ë‹µë³€ ë° ê·¼ê±°]")
        print(f"{'='*70}{Colors.ENDC}\n")

        print(f"{Colors.BOLD}[AI ì–´ì‹œìŠ¤í„´íŠ¸]{Colors.ENDC}\n")
        print(assistant_message)

        print(f"\n{Colors.HEADER}[ë‹µë³€ ê·¼ê±°]{Colors.ENDC}")
        metadata = result["metadata"]
        print(f"  - ì‚¬ìš©ëœ ì²­í¬: {metadata['num_chunks']}ê°œ")
        print(f"  - ì‚¬ìš©ëœ ê´€ê³„: {metadata['num_relations']}ê°œ")
        print(f"  - íƒìƒ‰ ê¹Šì´: 2-hop")
        print(f"  - ì‚¬ìš© í† í°: {metadata['tokens_used']}")
        print(f"  - ëŒ€í™” ì»¨í…ìŠ¤íŠ¸: {', '.join(list(self.context_terms)[-3:])}" if len(self.context_terms) > 1 else "  - ëŒ€í™” ì»¨í…ìŠ¤íŠ¸: ì—†ìŒ")

        print(f"\n{Colors.BOLD}{'â”'*70}{Colors.ENDC}\n")

    def run(self):
        """ëŒ€í™”í˜• ë£¨í”„ ì‹¤í–‰"""
        print(f"{Colors.HEADER}ğŸ’¬ ëŒ€í™”ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤. ì¢…ë£Œí•˜ë ¤ë©´ 'exit' ë˜ëŠ” 'quit'ë¥¼ ì…ë ¥í•˜ì„¸ìš”.{Colors.ENDC}\n")

        # í™˜ì˜ ë©”ì‹œì§€
        print(f"{Colors.OKGREEN}ì•ˆë…•í•˜ì„¸ìš”! PokoPoko ê²Œì„ì˜ ì§€ì‹ ê·¸ë˜í”„ ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.{Colors.ENDC}")
        print(f"{Colors.OKGREEN}ê²Œì„ ë©”ì¹´ë‹‰, ì´ë²¤íŠ¸, UX, ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§ì— ëŒ€í•´ ë¬¼ì–´ë³´ì„¸ìš”!{Colors.ENDC}")
        print(f"{Colors.WARNING}v3.0: Evidence-based ë‹µë³€ ìƒì„± + XML êµ¬ì¡°í™”ëœ ì»¨í…ìŠ¤íŠ¸{Colors.ENDC}\n")

        while True:
            try:
                # ì‚¬ìš©ì ì…ë ¥
                user_input = input(f"{Colors.OKCYAN}You: {Colors.ENDC}").strip()

                if not user_input:
                    continue

                if user_input.lower() in ['exit', 'quit', 'ì¢…ë£Œ']:
                    print(f"\n{Colors.OKGREEN}ëŒ€í™”ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤. ê°ì‚¬í•©ë‹ˆë‹¤!{Colors.ENDC}\n")
                    break

                # íŠ¹ìˆ˜ ëª…ë ¹ì–´
                if user_input.lower() == 'history':
                    print(f"\n{Colors.HEADER}[ëŒ€í™” íˆìŠ¤í† ë¦¬]{Colors.ENDC}")
                    for i, msg in enumerate(self.conversation_history, 1):
                        role = "ì‚¬ìš©ì" if msg['role'] == 'user' else "AI"
                        print(f"{i}. [{role}] {msg['content'][:50]}...")
                    print()
                    continue

                if user_input.lower() == 'context':
                    print(f"\n{Colors.HEADER}[ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ ìš©ì–´]{Colors.ENDC}")
                    print(f"{', '.join(self.context_terms)}\n")
                    continue

                # ëŒ€í™” ì²˜ë¦¬
                self.chat(user_input)

            except KeyboardInterrupt:
                print(f"\n\n{Colors.OKGREEN}ëŒ€í™”ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤. ê°ì‚¬í•©ë‹ˆë‹¤!{Colors.ENDC}\n")
                break
            except Exception as e:
                print(f"\n{Colors.FAIL}âŒ ì˜¤ë¥˜ ë°œìƒ: {e}{Colors.ENDC}\n")


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    chatbot = GraphRAGChatbotV3()
    chatbot.run()


if __name__ == "__main__":
    main()
